\section{Predicting Fix Templates}
\label{sec:templ-pred}

In this section, we introduce our high-level approach to predicting repair
templates $\T$ for a given location of the program. Our goal is to define the
$\evalsym$ function in \autoref{fig:api}, in terms of the simple language
\repairLang (\autoref{fig:syntax}), which takes a $\ModelT$ and a feature vector
$\V$ of a specific subexpression as an input and produces a confidence score
$\C$ for each of the chosen templates $\T$.

Firstly, a $\ModelT$ is produced by $\trainTsym$, which performs supervised
learning on a training set of feature vectors $\V$, each assigned a vector of
(boolean) labels $\B$ that represent the template $\T$ that ``repairs'' the
location that the feature vector represents. In our case, only one template can
be used to repair a given location in a program, thus, at most, only one slot of
the label vector $\B$ can be $\etrue$. This is equivalent to the
\emph{multi-class classification} problem, where the predictor models have to
learn to distinguish between several classes. Once trained, we can make
predictions on new inputs, producing template confidences $\Runit$ for each
template $\T$.

Our $\ModelT$s expect feature vectors $\V$ and boolean labels $\B$, both of a
fixed length for each specific location. Therefore, we define similarly to
$\extractsym$, the function $\extractTsym$ in \autoref{fig:api}. We use again
$\diffsym$ to get the set of changed expressions of a given program pair. Those
are then used by the function $\clustersym$ to get the repair templates by
grouping different expressions together based on some similarity metric and thus
reducing their number and making them more concrete. The $\extractTsym$
function, then, extracts $\featuresym$ from each subexpression, acquired by
$\diffsym$ but limited to the type-error slice (TODO: ref) and assigns the
boolean labels based on the templates $\T$ according to $\clustersym$, with only
one being $\etrue$ at a time

\input{syntax.tex}

% \subsection{Features}
% \label{sec:features}
% The first issue we must tackle is formulating our learning task in machine
% learning terms. We are given programs over \lang, but learning algorithms expect
% to work with \emph{feature vectors} $\V$ --- vectors of real numbers, where each
% column describes a particular aspect of the input. Thus, our first task is to
% convert programs to feature vectors.

% We choose to model a program as a \emph{set} of feature vectors, where each
% element corresponds an expression in the program. Thus, given the |sumList|
% program in \autoref{fig:sumList} we would first split it into its constituent
% sub-expressions and then transform each sub-expression into a single feature
% vector. We group the features into five categories, using \autoref{tab:sumList}
% as a running example of the feature extraction process.

% \mypara{Local syntactic features}
% These features describe the syntactic category of each expression $e$. In other
% words, for each production of $e$ in \autoref{fig:syntax} we introduce a feature
% that is enabled (set to $1$) if the expression was built with that production,
% and disabled (set to $0$) otherwise. For example, the \IsNil feature in
% \autoref{tab:sumList} describes whether an expression is the empty list $\enil$.

% We distinguish between matching on a list vs.\ on a pair, as this affects the
% typing derivation. We also assume that all pattern matches are well-formed ---
% \ie all patterns must match on the same type. Ill-formed match expressions would
% lead to a type error; however, they are already effectively localized to the
% match expression itself. We note that this is not a \emph{fundamental}
% limitation, and one could easily add features that specify whether a match
% \emph{contains} a particular pattern, and thus have a match expression that
% enables multiple features.

% \mypara{Contextual syntactic features}
% These are similar to local syntactic features, but lifted to describe the parent
% and children of an expression. For example, the \IsCaseListP feature in
% \autoref{tab:sumList} describes whether an expression's \emph{parent} matches on
% a list. If a particular $e$ does not have children (\eg a variable $x$) or a
% parent (\ie the root expression), we leave the corresponding features disabled.
% This gives us a notion of the \emph{context} in which an expression occurs,
% similar to the \emph{n-grams} used in linguistic models
% \citep{Hindle2012-hf,Gabel2010-el}.

% \mypara{Expression size}
% We also propose a feature representing the \emph{size} of each expression, \ie
% how many sub-expressions does it contain? For example, the \ExprSize feature in
% \autoref{tab:sumList} is set to three for the expression |sumList tl| as it
% contains three expressions: the two variables and the application itself. This
% allows the model to learn that, \eg, expressions closer to the leaves are more
% likely to be blamed than expressions closer to the root.

% \mypara{Typing features}
% A natural way of summarizing the context in which an expression occurs is with
% \emph{types}. Of course, the programs we are given are \emph{untypeable}, but we
% can still extract a \emph{partial} typing derivation from the type checker and
% use it to provide more information to the model.

% A difficulty that arises here is that, due to the parametric type constructors
% $\tfun{\cdot}{\cdot}$, $\tprod{\cdot}{\cdot}$, and $\tlist{\cdot}$, there is an
% \emph{infinite} set of possible types --- but we must have a \emph{finite} set
% of features. Thus, we abstract the type of an expression to the set of type
% constructors it \emph{mentions}, and add features for each type constructor that
% describe whether a given type mentions the type constructor. For example, the
% type $\tint$ would only enable the $\tint$ feature, while the type
% $\tfun{\tint}{\tbool}$ would enable the $\tfun{\cdot}{\cdot}$, $\tint$, and
% $\tbool$ features.

% We add these features for parent and child expressions to summarize the context,
% but also for the current expression, as the type of an expression is not always
% clear \emph{syntactically}. For example, the expressions |tl| and |sumList tl|
% in \autoref{tab:sumList} both enable \HasTypeList, as they are both inferred to
% have a type that mentions $\tlist{\cdot}$.
% %constructor.

% Note that our use of typing features in an ill-typed program subjects us to
% \emph{traversal bias} \citep{McAdam1998-ub}. For example, the |sumList tl|
% expression might alternatively be assigned the type $\tint$. Our models will
% have to learn good localizations in spite this bias (see
% \autoref{sec:evaluation}).

% \mypara{Type error slice}
% Finally, we wish to distinguish between changes that could fix the error, and
% changes that \emph{cannot possibly} fix the error. Thus, we compute a minimal
% type error \emph{slice} for the program (\ie the set of expressions that
% contribute to the error), and add a feature that is enabled for expressions that
% are part of the slice. The \InSlice feature in \autoref{tab:sumList} indicates
% whether an expression is part of such a minimal slice, and is enabled for all of
% the sampled expressions except for |tl|, which does not affect the type error.
% If the program contains multiple type errors, we compute a minimal slice for
% each error.

% In practice, we have found that \InSlice is a particularly important feature,
% and thus include a post-processing step that discards all expressions where it
% is disabled. As a result, the |tl| expression would never actually be shown to
% the classifier. We will demonstrate the importance of \InSlice empirically in
% \autoref{sec:feature-utility}.


% \subsection{Labels}
% \label{sec:labels}
% Recall that we make predictions in two stages. First, we use $\evalsym$ to
% predict for each subexpression whether it should be blamed, and extract a
% confidence score $\Runit$ from the $\Model$. Thus, we define the output of the
% $\Model$ to be a boolean label, where ``false'' means the expression
% \emph{should not} change and ``true'' means the expression \emph{should} change.
% This allows us to predict whether any individual expression should change, but
% we would actually like to predict the \emph{most likely} expressions to change.
% Second, we \emph{rank} each subexpression by the confidence $\Runit$ that it
% should be blamed, and return to the user the top $k$ most likely blame
% assignments (in practice $k=3$).


% We identify the fixes for each ill-typed program with an expression-level
% diff~\citep{Lempsink2009-xf}. We consider two sources of changes. First, if an
% expression has been removed wholesale, \eg if $\eapp{f}{x}$ is rewritten to
% $\eapp{g}{x}$, we will mark the expression $f$ as changed, as it has been
% replaced by $g$. Second, if a new expression has been inserted \emph{around} an
% existing expression, \eg if $\eapp{f}{x}$ is rewritten to
% $\eplus{\eapp{f}{x}}{1}$, we will mark the application expression $\eapp{f}{x}$
% (but not $f$ or $x$) as changed, as the $+$ operator now occupies the original
% location of the application.


\subsection{Learning Algorithm}
\label{sec:models}
\lstDeleteShortInline{|} % sigh...

Our goal is to train a \emph{classifier} given a training set $S : \List{\V
\times \List{\B}}$ of labeled examples, to predict a boolean array $b$ of
templates $\T$ given a input of features $v$. Essentially, though, we require
that the classifier outputs a \emph{confidence score} $\Runit$ for each of the
Top-$N$ templates chosen for each location of the type-error slice.

There are many learning algorithms to choose from, existing on a spectrum that
balances expressiveness with ease of training (and of interpreting the learned
model). In this section we consider a standard learning algorithm: \emph{neural
networks}. A thorough introduction to these techniques can be found in
introductory machine learning textbooks \citep[\eg][]{Hastie2009-bn}.

% Below we briefly introduce each technique by describing the rules it learns, and
% summarize its advantages and disadvantages. For our application, we are
% particularly interested in three properties -- expressiveness, interpretability
% and ease of generalization. Expressiveness measures how complex prediction rules
% are allowed to be, and interpretability measures how easy it is to explain the
% cause of prediction to a human. Finally ease of generalization measures how
% easily the rule generalizes to examples that are not in the training set; a rule
% that is not easily generalizable might perform poorly on an unseen test set even
% when its training performance is high.


\mypara{Neural Networks}
The model that we use is a type of neural network called a \emph{multi-layer
perceptron} (see \citealt{Nielsen2015-pu} for an introduction to neural
networks). A multi-layer perceptron can be represented as a directed acyclic
graph whose nodes are arranged in layers that are fully connected by weighted
edges. The first layer corresponds to the input features, and the final to the
output. The output of an internal node $v$ is
\[ h_v = g\,(\sum_{j \in N(v)}\!W_{jv} h_j ) \] where $N(v)$ is the set of nodes
in the previous layer that are adjacent to $v$, $W_{jv}$ is the weight of the
$(j, v)$ edge and $h_j$ is the output of node $j$ in the previous layer. Finally
$g$ is a non-linear function, called the activation function, which in recent
work is commonly chosen to be the \emph{rectified linear unit} (ReLU), defined
as $g(x) = \mathsf{max}(0,x)$ \citep{Nair2010-xg}. The number of layers, the
number of neurons per layer, and the connections between layers constitute the
\emph{architecture} of a neural network. In this work, we use relatively simple
neural networks which have an input layer, a single hidden layer and an output
layer.

A major advantage of neural networks is their ability to discover interesting
combinations of features through non-linearity, which significantly reduces the
need for manual feature engineering, and allows high expressivity. On the other
hand, this makes the networks particularly difficult to interpret and also
difficult to generalize unless vast amounts of training data are available.


\subsection{Multi-class Classification}
\label{subsec:multi-class}

\mypara{Assigning Templates as Labels}
The first step is to assign the correct labels to each input vector $v$ for the
training and testing process. These labels must represent the repair template
$\T$ that was used to fix the input program at a location $l$. If that location
was not changed, all boolean labels are set to $\efalse$. Therefore, for the
purpose of template prediction we have $N$-length vectors as labels for each
location $l$ in the type-error slice, that at most one slot can set to $\etrue$,
leading to a \emph{multi-class classification} problem. Next, we extend our
models from \autoref{sec:localization} to tackle this problem.

\mypara{Multi-class \dnn{}s}
As in \autoref{sec:localization} for error localization, we again use a deep
neural network for our template prediction $\ModelT$s. The output here is a
$N$-length vector, thus the output layer has $N$ nodes. For multi-class
classification problems solved with neural network techniques, usually a
\emph{softmax} function is used for the output layer
\citep[][]{Goodfellow-et-al-2016, Bishop-book-2006}. Softmax assigns decimal
probabilities to each class that must add up to 1.0. This additional constraint
helps training converge more quickly than it otherwise would.

For an output vector $y = (y_1, \dots, y_N) \in \R^{N}$, the standard softmax
function is defined as:
\[ \sigma(y)_i = \frac{e^{y_i}}{\sum_{j=1}^{N} e^{y_j}},\ for\ i = 1, \dots, N \]

The number of layers and the number of neurons per layer are again
hyper-parameters of the model, but we choose again a deep architecture with
three layers, and with each layer having more neurons than the respective
error-localization models, with the exact number needing some tuning.

% TODO: it feels like something is missing (beside the examples)
