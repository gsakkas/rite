\section{Predicting Fix Templates}
\label{sec:templ-pred}

In this section, we introduce our high-level approach to predicting repair
templates $\T$ for a given location of the program. More formally, our goal is
to define the $\repairsym$ function in \autoref{fig:api}, in terms of the simple
language \repairLang (\autoref{fig:rtl-syntax}), that produces an error location
confidence score and confidence scores for some predefined templates for each
subexpression in a program $e$. The $\repairsym$ function takes some feature
extraction functions $\featuresym$, a dataset $\Dataset$ of program pairs $e
\times e$, a finite list of fix templates $\T$ as inputs and uses internally
three more major functions.

Firstly, the $\extractTsym$ function does the feature extraction from the
program pair dataset and maps each subexpression of the original erroneous
program to a boolean vector. The first entry refers to the error locations that
the user fixed, \ie whether a subexpression changed in a program pair. Those
locations are acquired through the $\diffsym$ function. The rest of the boolean
vector has at most one $\etrue$ value, and corresponds to the fix template $\T$
that used to repair that subexpression.

The $\extractTsym$ function is applied to all the dataset program pairs and all
subexpression feature and label vectors are then grouped together into one
larger \emph{training} dataset. The training dataset is used by the $\trainTsym$
function to produce two predictive models, $\Model$ and $\ModelT$. $\Model$ is
used for error localization and $\ModelT$ for predicting fix templates.

Finally, the $\predictTsym$ function produces an error location confidence score
$\Runit$ and confidence scores $\Tmap{\Runit}$ for each of the selected fix
templates for a specific subexpression in a new program $e$ that we want to
finally repair. The $\predictTsym$ function expects as input the two generated
predictive models and a new feature vector $\V$, that corresponds to a
subexpression's feature vector acquired again by $\extractTsym$. It is then
applied to each subexpression in $e$'s type-error slice (TODO: ref) and
generates a mapping from program expressions to confidence scores $\Emap{(\Runit
\times \Tmap{\Runit})}$ as output of the top-level function $\repairsym$.

% Firstly, a $\ModelT$ is produced by $\trainTsym$, which performs supervised
% learning on a training set of feature vectors $\V$, each assigned a vector of
% (boolean) labels $\B$ that represent the template $\T$ that ``repairs'' the
% location that the feature vector represents. In our case, only one template can
% be used to repair a given location in a program, thus, at most, only one slot of
% the label vector $\B$ can be $\etrue$. This is equivalent to the
% \emph{multi-class classification} problem, where the predictor models have to
% learn to distinguish between several classes. Once trained, we can make
% predictions on new inputs, producing template confidences $\Runit$ for each
% template $\T$.

% Our $\ModelT$s expect feature vectors $\V$ and boolean labels $\B$, both of a
% fixed length for each specific location. Therefore, we define similarly to
% $\extractsym$, the function $\extractTsym$ in \autoref{fig:api}. We use again
% $\diffsym$ to get the set of changed expressions of a given program pair. Those
% are then used by the function $\clustersym$ to get the repair templates by
% grouping different expressions together based on some similarity metric and thus
% reducing their number and making them more concrete. The $\extractTsym$
% function, then, extracts $\featuresym$ from each subexpression, acquired by
% $\diffsym$ but limited to the type-error slice (TODO: ref) and assigns the
% boolean labels based on the templates $\T$ according to $\clustersym$, with only
% one being $\etrue$ at a time

\input{syntax.tex}

\subsection{Feature and Label Extraction}
\label{subsec:extract}
The first issue we must tackle is formulating our learning task in machine
learning terms. We are given programs over \repairLang, but learning algorithms
expect to work with \emph{feature vectors} $\V$ --- vectors of real numbers,
where each column describes a particular aspect of the input. Thus, our first
task is to convert programs to feature vectors with the $\extractTsym$ function.

Similarly to previous work \citep{Seidel:2017}, we choose to model a program as
a \emph{set} of feature vectors, where each element corresponds an expression in
the program. Thus, given the |mulByDigit| program in \autoref{fig:mulByDigit} we
would first split it into its constituent sub-expressions and then transform
each sub-expression into a single feature vector. We group the features into
five categories, using \autoref{fig:mulByDigit} as a running example of the
feature extraction process.

\mypara{Local syntactic features}
These features describe the syntactic category of each expression $e$. In other
words, for each production of $e$ in \autoref{fig:ml-syntax} we introduce a
feature that is enabled (set to $1$) if the expression was built with that
production, and disabled (set to $0$) otherwise. For example, the \IsNil feature
describes whether an expression is the empty list $\enil$.

% We distinguish between matching on a list vs.\ on a pair, as this affects the
% typing derivation. We also assume that all pattern matches are well-formed ---
% \ie all patterns must match on the same type. Ill-formed match expressions would
% lead to a type-error; however, they are already effectively localized to the
% match expression itself. We note that this is not a \emph{fundamental}
% limitation, and one could easily add features that specify whether a match
% \emph{contains} a particular pattern, and thus have a match expression that
% enables multiple features.

\mypara{Contextual syntactic features}
These are similar to local syntactic features, but lifted to describe the parent
and children of an expression. For example, the \IsCaseListP feature would
describe whether an expression's \emph{parent} matches on a list. If a
particular $e$ does not have children (\eg a variable $x$) or a parent (\ie the
root expression), we leave the corresponding features disabled. This gives us a
notion of the \emph{context} in which an expression occurs, similar to the
\emph{n-grams} used in linguistic models \citep{Hindle2012-hf,Gabel2010-el}.

\mypara{Expression size}
We also propose a feature representing the \emph{size} of each expression, \ie
how many sub-expressions does it contain?
% For example, the \ExprSize feature in \autoref{fig:mulByDigit} is set to four
% for the expression |mulByDigit i tl| as it contains four expressions: the three
% variables and the application itself.
This allows the model to learn that, \eg, expressions closer to the leaves are
more likely to be fixed than expressions closer to the root.

\mypara{Typing features}
A natural way of summarizing the context in which an expression occurs is with
\emph{types}. Of course, the programs we are given are \emph{untypeable}, but we
can still extract a \emph{partial} typing derivation from the type checker and
use it to provide more information to the model.

A difficulty that arises here is that, due to the parametric type constructors
$\tfun{\cdot}{\cdot}$, $\tprod{\cdot}{\cdot}$, and $\tlist{\cdot}$, there is an
\emph{infinite} set of possible types --- but we must have a \emph{finite} set
of features. Thus, we abstract the type of an expression to the set of type
constructors it \emph{mentions}, and add features for each type constructor that
describe whether a given type mentions the type constructor. For example, the
type $\tint$ would only enable the $\tint$ feature, while the type
$\tfun{\tint}{\tbool}$ would enable the $\tfun{\cdot}{\cdot}$, $\tint$, and
$\tbool$ features.

We add these features for parent and child expressions to summarize the context,
but also for the current expression, as the type of an expression is not always
clear \emph{syntactically}. For example, the expressions |tl| and
|mulByDigit i tl| in \autoref{fig:mulByDigit} both enable \HasTypeList, as they
are both inferred to have a type that mentions $\tlist{\cdot}$.

% Note that our use of typing features in an ill-typed program subjects us to
% \emph{traversal bias} \citep{McAdam1998-ub}. For example, the |mulByDigit i tl|
% expression might alternatively be assigned the type $\tint$. Our models will
% have to learn good localizations in spite this bias.

\mypara{Type error slice}
Finally, we wish to distinguish between changes that could fix the error, and
changes that \emph{cannot possibly} fix the error. Thus, we compute a minimal
type-error \emph{slice} for the program (\ie the set of expressions that
contribute to the error) and if the program contains multiple type-errors, we
compute a minimal slice for each error. We then have a post-processing step that
discards all expressions that are not included in those slices.


\mypara{Labels}
Recall that we use two predictive models, $\Model$ for error localization and
$\ModelT$ for predicting fix templates. Thus, we define the output of the
$\Model$ to be a boolean label, where ``false'' means the expression
\emph{should not} change and ``true'' means the expression \emph{should} change.
We also want to assign the correct labels for $\ModelT$. These labels must
represent the repair template $\T$ that was used to fix the input program at a
location $l$. If that location was not changed, all boolean labels are set to
``false''. Therefore, for the purpose of template prediction, we have a
fixed-length boolean vector that represents the fix template used for repairing
a subexpression at a location $l$. This vector has at most one slot set to
``true''.

\subsection{Training Predictive Models}
\label{subsec:train}
\lstDeleteShortInline{|} % sigh...

Our goal with the $\trainTsym$ function is to train two separate
\emph{classifiers} given a training set $S : \List{\V \times \B \times
\Tmap{\B}}$ of labeled examples, one used to predict error locations $\Model$
and the other to predict fix templates $\ModelT$ for an new input program $p$.
Essentially, though, we require that the error localization classifier outputs a
\emph{confidence score} $\Runit$ that represents the probability that the
subexpression in program $p$ it was applied to, is the error that needs to be
fixed. We also require that the fix template classifier outputs a confidence
score $\Runit$ for each of the selected fix templates that measures how sure the
classifier is that a given template can be used to repair the associated
location of the input program $p$.

There are many learning algorithms to choose from, existing on a spectrum that
balances expressiveness with ease of training (and of interpreting the learned
model). In this section we consider a standard learning algorithm: \emph{neural
networks}. A thorough introduction to these techniques can be found in
introductory machine learning textbooks \citep[\eg][]{Hastie2009-bn}.

% Below we briefly introduce each technique by describing the rules it learns, and
% summarize its advantages and disadvantages. For our application, we are
% particularly interested in three properties -- expressiveness, interpretability
% and ease of generalization. Expressiveness measures how complex prediction rules
% are allowed to be, and interpretability measures how easy it is to explain the
% cause of prediction to a human. Finally ease of generalization measures how
% easily the rule generalizes to examples that are not in the training set; a rule
% that is not easily generalizable might perform poorly on an unseen test set even
% when its training performance is high.


\mypara{Neural Networks}
The model that we use is a type of neural network called a \emph{multi-layer
perceptron} (see \citep{Nielsen2015-pu} for an introduction to neural networks).
A multi-layer perceptron can be represented as a directed acyclic graph whose
nodes are arranged in layers that are fully connected by weighted edges. The
first layer corresponds to the input features, and the final to the output. The
output of an internal node $v$ is
\[ h_v = g\,(\sum_{j \in N(v)}\!W_{jv} h_j ) \] where $N(v)$ is the set of nodes
in the previous layer that are adjacent to $v$, $W_{jv}$ is the weight of the
$(j, v)$ edge and $h_j$ is the output of node $j$ in the previous layer. Finally
$g$ is a non-linear function, called the activation function, which in recent
work is commonly chosen to be the \emph{rectified linear unit} (ReLU), defined
as $g(x) = \mathsf{max}(0,x)$ \citep{Nair2010-xg}. The number of layers, the
number of neurons per layer, and the connections between layers constitute the
\emph{architecture} of a neural network. In this work, we use relatively
\emph{deep neural networks} (\dnn) which have an input layer, three hidden layers and
an output layer.

% A major advantage of neural networks is their ability to discover interesting
% combinations of features through non-linearity, which significantly reduces the
% need for manual feature engineering, and allows high expressivity. On the other
% hand, this makes the networks particularly difficult to interpret and also
% difficult to generalize unless vast amounts of training data are available.

\mypara{Multi-class \dnn{}s}
While the above model is enough for error localization (since we have to select
from a binary option), some adjustments on the output layer must be applied. We
again use a deep neural networks for our template prediction $\ModelT$s, but the
output here is a $N$-length vector and therefore the output layer has $N$ nodes.
For multi-class classification problems solved with neural network techniques,
usually a \emph{softmax} function is used for the output layer
\citep{Goodfellow-et-al-2016, Bishop-book-2006}. Softmax assigns decimal
probabilities to each class that must add up to 1.0. This additional constraint
helps training converge more quickly than it otherwise would.

For an output vector $y = (y_1, \dots, y_N) \in \R^{N}$, the standard softmax
function is defined as:
\[ \sigma(y)_i = \frac{e^{y_i}}{\sum_{j=1}^{N} e^{y_j}},\ for\ i = 1, \dots, N \]

The number of neurons per layer are left as the model's hyper-parameters, but we
choose again a deep architecture with three layers, and with each layer having
more neurons than the respective error-localization models, with the exact
number needing some tuning.

% TODO: it feels like something is missing (beside the examples)


\subsection{Predicting Fix Templates}
\label{subsec:predict}

Our ultimate goal is to be able to pinpoint what parts of an erroneous program
should be repaired and what fix templates should be utilized for that purpose.
Therefore, the $\repairsym$ function uses $\predictTsym$ to predict all
subexpressions' confidence scores $\Runit$ to be an error location and
confidence scores $\Tmap{\Runit}$ for each of the selected fix templates. In
this section we show how all the functions in our high-level API in
\autoref{fig:api} are combined to produce a final list of confidence scores for
a new program $P$.

\mypara{The Prediction Algorithm}
Our algorithm firstly extracts the machine learning appropriate dataset $D_{ML}$
from the program pairs dataset $D$. For each program pair in $D$, the
$\extractTsym$ function returns a mapping from the erroneous program's
expression to a feature and label vector. The function
$\textsc{GetSubEsInTESlice}$ only keeps the expressions in the the type-error
slice and returns a list of the respective feature and label vectors, which is
then added in the $D_{ML}$ dataset. The $D_{ML}$ dataset is used by the
$\trainTsym$ function to generate our predictive $Models$. At this point we have
to perform again the feature extraction for a new program $P$, not included
before in our dataset $D$. Thus we use again $\extractTsym$, by providing
program $P$ twice, since in the case we don't have a program as we are trying to
produce the final repair. After acquiring again only the expression that we find
in the type-error slice and therefore ignoring the label vectors, we apply the
$\predictTsym$ function in each feature vector $v$, that corresponds to an
expression in the type-error slice of $P$. The predicted confidence scores are
then accumulated into the $EMap$ mapping, which is used by our synthesis
algorithm in \autoref{sec:synthesis} to correlate expressions in $P$ with their
confidence scores.

\input{predict-algo.tex}
