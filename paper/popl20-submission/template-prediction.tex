\section{Predicting Repair Templates}
\label{sec:templ-pred}

In this section, we extend our API from \autoref{sec:localization}, so we are
able to predict repair templates $\T$ for a given location of the program. Our
goal is to define the $\evalsym$ function in \autoref{fig:api}, in terms of the
simple language \repairLang (\autoref{fig:syntax}), which takes a $\ModelT$ and
a feature vector $\V$ of a specific subexpression as an input and produces a
confidence score $\C$ for each of the chosen templates $\T$. Any given template
$\T$ is an expression $e$ in the \repairLang, which is a simple lambda calculus
with integers, booleans, pairs, and lists.

Firstly, a $\ModelT$ is produced by $\trainTsym$, which performs supervised
learning on a training set of feature vectors $\V$, each assigned a vector of
(boolean) labels $\B$ that represent the template $\T$ that ``repairs'' the
location that the feature vector represents. In our case, only one template can
be used to repair a given location in a program, thus, at most, only one slot of
the label vector $\B$ can be $\etrue$. This is equivalent to the
\emph{multi-class classification} problem, where the predictor models have to
learn to distinguish between several classes. Once trained, we can make
predictions on new inputs, producing template confidences $\Runit$ for each
template $\T$.

Similarly to \autoref{sec:localization}, our $\ModelT$s expect feature vectors
$\V$ and boolean labels $\B$, both of a fixed length for each specific location.
Therefore, we define similarly to $\extractsym$, the function $\extractTsym$ in
\autoref{fig:api}. We use again $\diffsym$ to get the set of changed expressions
of a given program pair. Those are then used by the function $\clustersym$ to
get the repair templates by grouping different expressions together based on
some similarity metric and thus reducing their number and making them more
concrete. The $\extractTsym$ function, then, extracts $\featuresym$ from each
subexpression, acquired by $\diffsym$ but limited to the type-error slice (TODO:
ref) and assigns the boolean labels based on the templates $\T$ according to
$\clustersym$, with only one being $\etrue$ at a time


\subsection{RTL: Repair Template Language}
\label{subsec:lang}

In this section, we define the language we are going to use to represent
templates and how we can acquire those templates from our dataset.

\mypara{Syntax of RTL}
We define here our Repair Template Language (RTL) as a simple lambda calculus
(\repairLang in \autoref{fig:syntax}) with integers, booleans, pairs, and lists.
This language will be used to abstract and define repair templates $\T$. Our RTL
contains all the expressions that \lang included, but with four significant
changes.

\begin{enumerate}
    \item Variable names $x$ used for functions, variables and patterns are left
    \emph{unspecified}. That means that $x$ denotes that that place of the code
    has a variable name, but it is unknown at that point.
    \item Literal values $n$ can be any integer or floating point number,
    boolean value or character and string. Same as variables, these are left
    unspecified.
    \item Operators $\oplus$ are also left unspecified.
    \item A \emph{wildcard} expression $\_$ is added, which is used to denote
    that \emph{any} expression in \repairLang can be used to replace it.
\end{enumerate}

\input{syntax.tex}

\mypara{Generic ASTs (GASTs)}
Using our RTL, we can now define \emph{Generic ASTs}. GASTs are abstract syntax
trees that represent expressions that can have unspecified details, like
variable names or numerical values. Therefore, GASTs can represent templates
$\T$, defined as expressions in \repairLang. GASTs are used here to capture the
high-level \emph{structure} of an expression, as a more appropriate means to
define templates. GASTs are extracted from the original ASTs representing
programs in \lang, by removing all the necessary information so \repairLang can
be used. Then these trees can be further \emph{pruned} at a depth $d$ to keep
only the higher level expressions and formulate the templates.


\mypara{Extracting Templates from Dataset Repairs}
Templates $\T$ are extracted by the function $\templatesym$ from our program
pair dataset. Using the $\diffsym$ function first, we acquire the changed
expressions of the dataset. These changes are potentially the templates we are
going to use. However, they contain a lot of \emph{local} information to the
specific program that they were extracted from. So $\templatesym$ takes the
changed expressions defined in \lang and \emph{transforms} them into expressions
in \repairLang, essentially making them into templates $\T$. Those templates are
represented as GASTs and are pruned at a pre-defined depth $d$. The value of $d$
can be as low as 1, but to capture more structure to our templates, slightly
bigger values would work better. We then see in \autoref{subsec:clustering} how
we can group together all these templates.

\mypara{Example}
TODO: give an example

\subsection{Clustering the Templates}
\label{subsec:clustering}

% TODO: Should we give a better name than "similarity metric"
\mypara{GAST Similarity Metric}
Having programs written over \repairLang, forces similar changes, \ie changing a
variable name, to have the same GAST. This is really helpful when we try to
group different changes together into some templates $\T$. We want to extend and
generalize this ``similarity'' of program changes, in order to get a small but
generally applicable number of repair templates.

Therefore, two expressions over \repairLang are considered \emph{similar} when
the following simple rules hold:
\begin{enumerate}
    \item Their pruned GASTs (at a pre-defined depth $d$) are exactly the same.
    \item A wildcard expression is considered similar with any possible
    expression over \repairLang.
    \item Two applications, operators, cases, lists or tuples are considered
    similar when there is a total bijective relation between their children
    expressions, such that the related expressions are themselves similar.
    % TODO: maybe explain better and separate apps, and include ifs and lets
\end{enumerate}


\mypara{Clustering}
The main goal of $\clustersym$ing is to eliminate the possibility of equivalent
expressions to be considered as different templates and thus making our
$\ModelT$s more scalable and applicable to more programs. We define
$\clustersym$ing as the task of grouping together similar expressions over
\repairLang that each group can later be used as a template $\T$ to produce
repairs for ill-typed programs. Each group can consist of several
\emph{member-expressions} and each one of them can be used as the cluster
\emph{representative}.

The $\clustersym$ function uses the templates produced by $\templatesym$ and
groups together the templates that are similar based on our ealrier definition
of expression similarity. It then returns the Top-$N$ based on their popularity
on their training set. $N$ is considered a parameter of algorithm that can be
defined prior the training and clustering process, and is usually around 30 to
50 [TODO: cite similar template papers].

\mypara{Example}
TODO: give an example


\subsection{Multi-class Classification}
\label{subsec:multi-class}
Our goal is to train a \emph{classifier} given a training set $S : \List{\V
\times \List{\B}}$ of labeled examples, to predict a boolean array $b$ of
templates $\T$ given a input of features $v$. Essentially, though, we require
that the classifier outputs a \emph{confidence score} $\Runit$ for each of the
Top-$N$ templates chosen for each location of the type-error slice.

\mypara{Assigning Templates as Labels}
The first step is to assign the correct labels to each input vector $v$ for the
training and testing process. These labels must represent the repair template
$\T$ that was used to fix the input program at a location $l$. If that location
was not changed, all boolean labels are set to $\efalse$. Therefore, for the
purpose of template prediction we have $N$-length vectors as labels for each
location $l$ in the type-error slice, that at most one slot can set to $\etrue$,
leading to a \emph{multi-class classification} problem. Next, we extend our
models from \autoref{sec:localization} to tackle this problem.

\mypara{Multi-class \dnn{}s}
As in \autoref{sec:localization} for error localization, we again use a deep
neural network for our template prediction $\ModelT$s. The outputhere is a
$N$-length vector, thus the output layer has $N$ nodes. For multi-class
classification problems solved with neural network techniques, usually a
\emph{softmax} function is used for the output layer
\citep[][]{Goodfellow-et-al-2016, Bishop-book-2006}. Softmax assigns decimal
probabilities to each class that must add up to 1.0. This additional constraint
helps training converge more quickly than it otherwise would.

For an output vector $y = (y_1, \dots, y_N) \in \R^{N}$, the standard softmax
function is defined as:
\[ \sigma(y)_i = \frac{e^{y_i}}{\sum_{j=1}^{N} e^{y_j}},\ for\ i = 1, \dots, N \]

The number of layers and the number of neurons per layer are again
hyper-parameters of the model, but we choose again a deep architecture with
three layers, and with each layer having more neurons than the respective
error-localization models, with the exact number needing some tuning.

% TODO: it feels like something is missing (beside the examples)
