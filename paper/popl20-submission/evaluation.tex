\section{Evaluation}
\label{sec:eval}

We have implemented our technique for repairing type errors for a purely
functional subset of \ocaml with polymorphic types and functions. We seek to the
following \emph{Research Questions}.

\begin{itemize}
    \item \textbf{RQ1}: What is the \emph{quality} of \toolname's repairs?
    \item \textbf{RQ2}: Does \toolname \emph{localize} correctly errors?
    \item \textbf{RQ3}: Do our ML models \emph{generalize} well for error
    localization and template prediction?
    \item \textbf{RQ4}: Are \toolname's repairs actually useful for feedback
    generation to novice programmers?
\end{itemize}

We first present our experimental methodology (\autoref{subsec:gen_method}) and
then we will try to answer each of the above questions, drawing from our results
from a human study and our manual evaluation.


\subsection{General Methodology}
\label{subsec:gen_method}
To answer our questions, we will use an \ocaml dataset gathered from an
undergraduate Programming Languages course at UNKNOWN UNIVERSITY, previously
used in related work [FIXME: cite later]. This dataset consists of ill-typed
programs and their subsequent fixes, drawn from two different years of that
class, The first part of the dataset comes from the Spring 2014 class (\SPRING),
with a cohort of 46 students and the second comes from the Fall 2015 class
(\FALL), with a cohort of 56 students. There were totally 23 distinct programs
that the students worked on this homework. While the extracted programs are
relatively small, they demonstrate a range of functional programming idioms, \eg
higher-order functions and (polymorphic) algebraic data types.

\mypara{Feature Extraction}
We extract 416 features from each sub-expression in a
program, including:
%
\begin{enumerate}
  \item 45 local syntactic features.
  \item 315 contextual syntactic features. For each sub-expression we
    additionally extract the local syntactic features of its first, second,
    third and fourth (left-to-right) children. In addition, we extract those
    features for its ancestors, starting from its parent and going up to two
    more parent nodes. If an expression does not have a ancestor or children,
    these features will simply be disabled. If an expression has more than four
    children, the classifiers will receive no information about the additional
    children.
  \item 88 typing features. We support |int|s, |float|s, |char|s, |string|s, and
    the user-defined |expr|. These features are extracted for each
    sub-expression and its context.
  \item 1 feature denoting the size of each sub-expression.
\end{enumerate}

\mypara{Dataset ``cleaning''}
We automatically extract a blame oracle for each ill-typed program from the
(AST) diff between it and the student's eventual fix. A disadvantage of using
diffs in this manner is that students may have made many, potentially unrelated,
changes between compilations; at some point the ``fix'' becomes a ``rewrite''.
We do not wish to consider the ``rewrites'' in our evaluation, so we discard
outliers where the fraction of expressions that have changed is more than one
standard deviation above the mean, establishing a diff threshold of 40\%. We
also discard programs that changed in 5 or more locations, since such changes
can be small in expression size, they can still be considered a ``rewrite''. It
is also highly unlikely that such ``fixes'' can reproduced by \toolname or any
related work [FIXME: cite something relevant]. All these discarded outliers
account for roughly 32\% of each dataset, leaving us with 2,475 program pairs
for \SPRING and 2,177 pairs for \FALL. For all of our tests, we use \SPRING as a
training set and \FALL as a test set.

\mypara{Accuracy Metrics}
A recent study of fault localization techniques \citep[][]{Kochhar2016-oc} shows
that most developers will not consider more than around five potential error
locations before falling back to manual debugging. We evaluate \toolname on
whether a changed expression occurred in its top one, top three or top six
predictions, but our automatic method of producing fixes may consider more than
that. We also extend such intuition, that a user won't consider more than five
``suggestions'' as feedback, to possible fixes, and thus we evaluate \toolname's
top five template predictions on whether they include the correct template. We
also include the confusion matrix of the top one predictions for all locations,
in order to present what templates our models usually mix together.

\mypara{Repair Quality}
Finally, for a more qualitative evaluation of \toolname in order to evaluate the
synthesized solutions based on the above results, we ran a human study at
UNKNOWN UNIVERSITY. Each participant was asked to evaluate the quality of the
program fixes and their locations against \seminal's repairs
\citep[][]{Lerner2006-pj, Lerner2007-dt}. For each program, beside the two
repairs, the participants were given the original ill-typed program, along with
the standard \ocaml compiler's error message and a short description of what the
original author of the program intended it to do.

\subsection{Quantitative Evaluation}
\label{subsec:quan_eval}

First, we evaluate the accuracy of our predictions for both type error localization and template prediction.

\subsubsection{Error Localization Accuracy}
\label{subsubsec:error_loc_acc}



\subsubsection{Template Prediction Accuracy}
\label{subsubsec:templ_acc}

\mypara{Baselines}
We provide two baselines for the comparison: a random choice of template and a
prediction based on popularity.

\mypara{Our Classifiers}
We evaluate our classifiers, each trained on the \SPRING quarter and evaluated
on the \FALL quarter.

These include:
%Our classifiers are:
%
\begin{description}
  \item[\random] A dummy classifier that \emph{uniformly} selects each time a
    template to return. There is no training necessary with this classifier.
  \item[\popular] This classifier returns always the most popular template from
    the training set. As a second prediction returns the second most popular
    template and so on. The training phase of this classifier includes just
    learning the popularity of each cluster-template on the training set.
  \item[\textsc{Deep Neural Network} (\dnn)] A multi-layer perceptron neural
    network, trained with $\eta = 0.001$, $\lambda = 0.001$, and a mini-batch
    size of 256. Our network utilizes three fully-connected hidden layers of 512
    neurons The neurons use rectified linear units (ReLU) as their activation
    function, a common practice in modern neural networks.
\end{description}

All classifiers were trained using the \emph{early stopping} approach, where the
training of a neural network is stopped when the accuracy on a distinct small
part of the training set is not improved after a certain amount of epochs. We
set that amount to 5 and the maximum number of epochs to 200. The \textsc{Adam}
optimizer \citep{Kingma2014-ng}, a variant of stochastic gradient descent that
has been found to converge faster, was used to train our \dnn.

\input{evaluation-accuracy-graph}

\mypara{Results}
\autoref{fig:accuracy-results} shows the accuracy results of our template
prediction experiments. The naive baseline of selecting templates at random
achieves \RandomTopOne\% Top-1 accuracy (\RandomTopSix\% Top-6), while the
\popular classifier achieves a Top-1 accuracy of \PopularTopOne\%
(\PopularTopSix\% Top-6). Our \dnn classifier of course outperforms those naive
classifiers, ranging from \DnnTopOne\% Top-1 accuracy to \DnnTopSix\% Top-6
accuracy. Interestingly, with only \dnn's first prediction, one outperforms top
6 predictions of both \random and \popular.

The \random classifier selects \emph{randomly} a template, out of the 50 learned
ones from the \SPRING training dataset, for each location it is presented. It
was expected, therefore, to achieve such low accuracy, even for Top-6.
Surprisingly, the \popular accuracy results are not that low. This mainly
happens because of the nature of our dataset. Some homework assignments were
shared between \SPRING and \FALL quarters and, while different groups of
students solved these problems for each quarter, the novice mistakes that they
made seem to have a \emph{pattern}. Thus, the most \emph{popular ``fixes''} (and
therefore the relevant templates) that were applied to \SPRING, are also popular
in \FALL. However, the almost \emph{2x higher} accuracy of our \dnn classifier,
shows that we can learn a \emph{better pattern} from these datasets and that our
\dnn model is able to \emph{generalize} between different solutions of the
problems.


\subsubsection{Empirical Repair Quality Evaluation}
\label{subsubsec:man_rep_qual_eval}



\subsection{Qualitative Evaluation}
\label{subsec:quan_eval}

\subsubsection{Human Study Setup}
\label{subsubsec:study_setup}


\subsubsection{Human Study Results}
\label{subsubsec:study_res}
